<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!-- Hi, Jon Here. Please DELETE the two <script> tags below if you use this HTML, otherwise my analytics will track your page -->
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-7580334-2"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-7580334-2');
  </script>

  <title>Train-Skill-Embeddings</title>
  
  <meta name="author" content="Jon Barron">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Practical Approach to Word Embeddings using Word2Vec and Genism</name>
              </p>
              <p> By <i> Harsh Panwar </i>
              </p>
              <p> Dec 15,2019 - 5 min read </p>
              
              <p style="text-align:center">
                <a href="mailto:harshpanwar@ieee.org">Email</a> &nbsp/&nbsp
                <a href="https://github.com/Harsh9524/Harsh9524.github.io/blob/master/data/CV_Harsh_Panwar.pdf">CV</a> &nbsp/&nbsp
                <a href="https://www.researchgate.net/profile/Harsh_Panwar2">ResearchGate</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=g0UG174AAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://www.github.com/Harsh9524">Github</a> &nbsp/&nbsp
                <a href="https://twitter.com/harsh9524">Twitter</a>
                <!-- <a href="https://www.linkedin.com/in/harsh-panwar/">LinkedIn</a> -->
              </p>
            </td>
<!--             <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/JonBarron.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/dp_circle.png" class="hoverZoomLink"></a>
            </td> -->
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Introduction</heading>
              <p>
                Word Embeddings are used to make a relation between a bag of words which are related to each other in some
sense. For instance if there is a dataset of 100 candidates containing skills of each candidate in the form of a list, using word embeddings we can make a relation between each word so that Java and C++ are more related than Java and Business Development.
Word embedding is the collective name for a set of language modeling and feature learning techniques in natural language processing (NLP) where words or phrases from the vocabulary are mapped to vectors of real numbers. Conceptually it involves a mathematical embedding from a space with many dimensions per word to a continuous vector space with a much lower dimension.

So with the help of a practical example I will try to explain the concepts of Word Embeddings. I have used Word2Vec as the algorithm implemented through Gensim. The full Github repo is available <a> <href="https://github.com/Harsh9524/Train-Skill-Embeddings"> here </a>.

                <!-- <span class="highlight">highlighted</span>. -->
              </p>
            </td>
          </tr>
        </tbody></table>
       
       <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Task</heading>
              <p>
               We are given a dataset which is in the form:
                <p>
                  <i> <b> [[skill1,skill2,skill3,...],[skill1,skill4,skill6,...],....] </b> </i>
                <!-- <span class="highlight">highlighted</span>. -->
              </p>
                Each list inside the main list is of a unique candidate. So the number of candidates is equal to the length of the outer list. Which in our dataset is around 3 million. We want to learn an embedding representation of each skill and plot it using TensorBoard to analyze. The idea to solve is of word embeddings where words of similar meaning tend to lie close to each other for example red and black lie closer to each other in the embedding space than semantically diTerent words like black and apple. Since our dataset contains skills instead of general words skills like Python and Java should lie closer to each other than say Python and Sales Management.
              </p>
            </td>
          </tr>
        </tbody></table>
        
        
        
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Data Pre-Processing</heading>
              <p>
               Now we have to clean our data and preprocess it since our data is very big and unstructured so it cannot be understood by our model in this format.

Firstly, since our data is in .txt format we need to open it using the with open command in python as follows:

                <p style="background-color:rgb(224,224,224);">
                <i>  &nbsp;&nbsp; with open(“all_linkedin_skill_data”,mode=’r’) as i, open(‘out.txt’,’w’) as    
          <br> &nbsp;&nbsp;&nbsp;&nbsp; x = i.read(1)
                  <br>
 </i>
                <!-- <span class="highlight">highlighted</span>. -->
                </p>
                Then to organize the data and make it more understandable we can use one of the following approaches to clean the data:
                <br> <i>Thanks to Uwe Ziegenhagen for answering my question on stackover8ow and giving me this better approach to process the data. </i> <br>
                 </p>
              <p style="background-color:rgb(224,224,224);">
               <i> 
&nbsp;&nbsp;while True:
<br>&nbsp;&nbsp;&nbsp;&nbsp; x = i.read(1)
<br> &nbsp;&nbsp;if x == ‘’: # end of file has been reached break
<br>&nbsp;&nbsp;&nbsp;&nbsp; elif x==’ ‘:
<br>&nbsp;&nbsp;&nbsp;&nbsp; pass
<br>&nbsp;&nbsp;&nbsp;&nbsp; elif x==’]’:
<br>&nbsp;&nbsp;&nbsp;&nbsp; pass
<br>&nbsp;&nbsp;&nbsp;&nbsp; elif x==’[‘:
<br>&nbsp;&nbsp;&nbsp;&nbsp; if lastreadchar == ‘[‘:
<br>&nbsp;&nbsp;&nbsp;&nbsp; # at the beginning of the file, don’t do anything pass
<br>&nbsp;&nbsp;&nbsp;&nbsp; elif lastreadchar == ‘\n’: # a new line pass
<br>&nbsp;&nbsp;&nbsp;&nbsp; elif lastreadchar == ‘,’: # a new line pass
<br>&nbsp;&nbsp;&nbsp;&nbsp; elif x==’,’:
<br>&nbsp;&nbsp;&nbsp;&nbsp; if lastreadchar == ‘]’: # at the beginning of the file
<br>&nbsp;&nbsp;&nbsp;&nbsp; o.write(‘\n’) 
        <br>&nbsp;&nbsp;&nbsp;&nbsp;         else:
<br> &nbsp;&nbsp;&nbsp;&nbsp;o.write(x) 
          <br>&nbsp;&nbsp;&nbsp;&nbsp;       else:
<br>&nbsp;&nbsp;&nbsp;&nbsp; o.write(x) 
             <br>&nbsp;&nbsp;&nbsp;&nbsp;    lastreadchar = x
                </i>
              </p>
          <p>Or you can also use a simpler approach which I wrote myself </p>
          <p style="background-color:rgb(224,224,224);">
            <i> 
              
            &nbsp;&nbsp;  with open('file.txt') as f:
              <br> &nbsp;&nbsp;&nbsp;&nbsp;mylist = list(f)
  <br> &nbsp;&nbsp;&nbsp;&nbsp;temp = mylist[0]
<br> &nbsp;&nbsp;&nbsp;&nbsp; l =	temp.split(']')
        <br> &nbsp;&nbsp;&nbsp;&nbsp;      l.pop(0)
 <br> for x in range(0,len(l)): 
              <br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; l[x] = l[x][3:]
<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; l[x] = l[x].split(', ')
<br>

            </i> 
          </p>
          <p> If you have any doubt regarding the preprocessing of this data please refer to this <a> <href=" https://stackoverflow.com/questions/59325991/how-to-open-a-big-text-file-which-is-in-the-format-skill1-skill2-skill3">Stackoverflow </a> thread. </p>
            
            </td>
          </tr>
        </tbody></table>
        
  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Word2Vec</heading>
              <p> 
                Word2Vec is of two types CBOW (Continuous Bag of Words) or continuous skip gram.It can be understood as a two layered NN (Neural Net) that takes a corpus of text as input and return a vector. It can not be considered a Deep Neural Network but it is able to turn texts into numeric form understandable by deep nets.
                
              </p>
              <p> <i> Word2vec is a group of related models that are used to produce word embeddings. These models are shallow, two-layer neural networks that are trained to reconstruct linguistic contexts of words. </i> </p>
              <p>Now we are going to implement CBOW model Xrst and then later on we will implement the Skip Gram model. </p>
             
              <p style="background-color:rgb(224,224,224);"> 
                
              &nbsp;&nbsp; if not os.path.exists(‘model_out’):
              <br>&nbsp;&nbsp;&nbsp;&nbsp; model1 = gensim.models.Word2Vec(l, min_count = 1, size = 100, window = 5)
              <br> &nbsp;&nbsp;&nbsp;&nbsp;model1.save(‘model_out’)
              <br> &nbsp;&nbsp;skill1 = input(“Enter first skill:”).lower() 
              <br>&nbsp;&nbsp; skill2 = input(“Enter second skill:”).lower()
              <br>&nbsp;&nbsp; model_new = Word2Vec.load('model_out')
              <br>&nbsp;&nbsp; print(model_new.similarity(skill1,skill2))
              <br>
              </p>
              
              <p> An open source library known as Gensim which is built for implementing unsupervised machine learning algorithms can be used here to implement our Word2Vec models. It can be simply installed by using pip.
              </p>
              <p> <i> <b> pip install Genism </b> </i> </p> 
              <p> after importing the Word2Vec model from te Gensim .models we can directly use the Word2Vec() method for training our model. We can change our parameters by Hit and Trial to obtain better results. </p>
              <p > <i> <b> from genism.models import Word2Vec </b> </i> </p> 
              <p> To obtain the similarity of diTerent skills we can use the model.similarity(skill1,skill2) method which will give out the result in percentage. </p>
              
              </td>
          </tr>
        </tbody></table>
        
        
  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Visualisation using TensorBoard</heading>
              <p>
                <i> TensorBoard provides the visualisation and tooling needed for machine learning experimentation. </i> </p>
              <p> The following code can be used for visualising the data with the help of TensorBoard projector which creates a mesh of words using PCA and each word is connected to other by calculating their Cosine similarities.</p>
              <p style="background-color:rgb(224,224,224);">
                <i> 
                &nbsp;&nbsp; model2 = gensim.models.keyedvectors.KeyedVectors.load(‘model_out’)
                <br>&nbsp;&nbsp; max_size = len(model2.wv.vocab)-1
<br> &nbsp;&nbsp;w2v = np.zeros((max_size,model2.layer1_size))
<br> &nbsp;&nbsp;if not os.path.exists(‘projections’):
<br> &nbsp;&nbsp;&nbsp;&nbsp; os.makedirs(‘projections’)
<br> &nbsp;&nbsp;with open(“projections/metadata.tsv”,”w+”) as file_metadata: 
<br> &nbsp;&nbsp;&nbsp;&nbsp; for i, word in enumerate(model2.wv.index2word[:max_size]):
<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp; w2v[i] = model2.wv[word]
<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp; file_metadata.write(word + ‘\n’) 
                  &nbsp;&nbsp;&nbsp;&nbsp;<br> sess = tf.InteractiveSession() with tf.device(“/cpu:0”):
<br> &nbsp;&nbsp;&nbsp;&nbsp; embedding = tf.Variable(w2v, trainable=False, name=’embedding’) 
               &nbsp;&nbsp;&nbsp;&nbsp;  <br>  tf.global_variables_initializer().run()
<br>&nbsp;&nbsp;saver = tf.train.Saver()
<br>&nbsp;&nbsp;writer = tf.summary.FileWriter(‘projections’,sess.graph) 
               &nbsp;&nbsp;&nbsp;&nbsp;  <br>  config = projector.ProjectorConfig()
<br>&nbsp;&nbsp;embed = config.embeddings.add()
<br>&nbsp;&nbsp;embed.tensor_name = ‘embedding’
<br> &nbsp;&nbsp;embed.metadata_path = ‘metadata.tsv’
               &nbsp;&nbsp;&nbsp;&nbsp;  <br>  projector.visualize_embeddings(writer, config)
             &nbsp;&nbsp;&nbsp;&nbsp;     <br> saver.save(sess, ‘projections/model.ckpt’, global_step=max_size)
              &nbsp;&nbsp;   &nbsp;&nbsp;  <br> </i> </p>
             
              <p> This is a little bit complex and hard to understand but have a look at my Github repo here for better understanding and raise a issue if you are struck anywhere. After we have successfully created our code for the implementation of TensorBoard the output of it can be generated by the following line of code. </p>
              <p style="text-align:center;background-color:rgb(224,224,224);">
                <i> 
                  tensorboard --logdir=projections --port=8000 </i> </p>
              <p> 
                This will create a localhost server accessable at http://localhost:8000/. </p>
              
            </td> 
          </tr>
        </tbody></table>
  
  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Conclusion</heading>
              <p>
                Word Embeddings can be helpful in creating many NLP related models such as chatbot, text translator, documents similarities, plagiarism detection. Through establishing a connection between the words our Machine learning model can better understand natural language and it's intents.
              </p>
              <p> 
                The full code repo is available on my Github account <i> <a> <href="https://github.com/Harsh9524/Train-Skill-Embeddings"> here </a></i>.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
